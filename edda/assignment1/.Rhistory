xlab = xlab)
hist(secB, freq = FALSE,
main = "Histogram of p, sd=5\nn=m=100",
xlab = xlab)
hist(secC, freq = FALSE,
main = "Histogram of p, sd=15\nn=m=30",
xlab = xlab)
mich_1879 = read.table("light1879.txt")
mich_1882 = read.table("light1882.txt", fill=TRUE)
newcomb =read.table("light.txt")
data_mich_1879 = data.matrix(mich_1879)
data_mich_1882 = data.matrix(mich_1882)
data_newcomb = data.matrix(newcomb)
# We determine  normality by plotting a histogram and QQ Plot for every dataset.
par(mfrow=c(1,2))
hist(data_mich_1879, main="Historgram of data:\nMichelson 1879")
qqnorm(data_mich_1879, main="Q-Q Plot of data:\nMichelson 1879")
par(mfrow=c(1,2))
hist(data_mich_1882, main="Historgram of data:\nMichelson 1882")
qqnorm(data_mich_1882, main="Q-Q Plot of data:\nMichelson 1882")
par(mfrow=c(1,2))
hist(data_newcomb, main="Historgram of data:\nNewcomb")
qqnorm(data_newcomb, main="Q-Q Plot of data:\nNewcomb")
# First off, the measurements have to be transformed to km/sec following the calculation below:
data_mich_1879_km_s = data_mich_1879 + 299000
data_mich_1882_km_s = data_mich_1882 + 299000
data_newcomb_km_s = 7.442/(((data_newcomb/1000)+24.8)/1000000)
# Lastly, we calculate the confidence intervals for the speed of light in km/sec for all three data sets using the One Sample t-test:
t.test(data_mich_1879_km_s)[[4]]; t.test(data_mich_1882_km_s)[[4]]; t.test(data_newcomb_km_s)[[4]]
# From Wikipedia, we learn that the currently most accurate value for the speed of light equals 299792.458 km/s.
speed_of_light = 299792.458
t.test(data_mich_1879_km_s)[[4]][1]<=speed_of_light & speed_of_light<=t.test(data_mich_1879_km_s)[[4]][2]; t.test(data_mich_1882_km_s)[[4]][1]<=speed_of_light & speed_of_light<=t.test(data_mich_1882_km_s)[[4]][2]; t.test(data_newcomb_km_s)[[4]][1]<=speed_of_light & speed_of_light<=t.test(data_newcomb_km_s)[[4]][2]
# Read telephone.txt
tel <- read.table(file = "telephone.txt", header = TRUE)
# Divide row 2 for histogram and boxplot
par(mfrow=c(1,3))
# Plotting telephone.txt
hist(x = tel$Bills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
boxplot(x = tel$Bills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
qqnorm(tel$Bills)
# Extract zero bills from the data
nonZeroBills <- tel[tel$Bills != 0, ]
# Divide row 2 for histogram and boxplot
par(mfrow=c(1,3))
# Plotting telephone.txt
hist(x = nonZeroBills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
boxplot(x = nonZeroBills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
qqnorm(nonZeroBills)
# We use as test statistic the median of the data
t = median(x=tel$Bills)
print(paste("The median equals", t))
# Set the simulation repeat
B = 1000
# Create T* array
tstar = numeric(length = B)
# Assign sample length
n = length(tel$Bills)
# Assign lambda vector
lamb = sample(c(0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07,
0.08, 0.9, 0.1), replace = TRUE, size = n)
# Simulate B times
for (i in 1:B) {
# Get sample from Exp(lambda)
xstar = rexp(n, rate = sample(lamb, 1))
# Receives tstar by the test statistics the median
tstar[i] = median(xstar)
}
# Draw histogram
hist(tstar, prob=T, col = "grey",
main = "Histogram of T*",
xlab = "T* [T star]")
# Draw density curve of T
lines(density(tstar), col="blue", lwd=2) # add a density estimate with defaults
lines(density(tstar, adjust=2), lty="dotted", col="darkgreen", lwd=2)
axis(1, t, expression(paste("t") ) )
# P left
pl = sum(tstar < t) / B
# P right
pr = sum(tstar > t) / B
p = 2 * min(pl, pr)
print(paste("The p value is", p, "and H0 is not rejected"))
confLvl <- t.test(tel$Bills)[[4]]
print(paste("%95 confidence interval for the sample: [",
confLvl[1], ",", confLvl[2], "]"))
# Align two histogram
par(mfrow = c(1,2))
# Distribution type
lambda <- 0.2
# Sample size
n <- 150
# Trials
rows <- 1000
# Simulate
sim <- rexp(n*rows, lambda)
# Plot histogram
hist(sim, main = "Exp(0.2) Distribution",
xlab = "Value")
# Create matrix
m <- matrix(sim, rows)
# Calculate sample means
sample.means <- rowMeans(m)
# Calculate mean and standard deviation
# of the simulation for CLT(Central Limit Theorem)
sm.avg <- mean(sample.means); sm.sd <- sd(sample.means)
# Plot average
hist(sample.means,
main = "Central Limit Theorem\nfor the sample mean",
xlab = "Means")
# Theoretical(Expected) standard deviation
sm.sd.clt <- sqrt(lambda/n)
# Average of the simulation, standard deviation
# and expected standard deviation
sm.avg; sm.sd; sm.sd.clt
# Calculation of %95 confidence level for the mean
confLvl <- t.test(sample.means)[[4]]
print(paste("%95 confidence interval for the sample: [",
confLvl[1], ",", confLvl[2], "]"))
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
binom.test(cond, length(tel$Bills), alternative = "less")[3][6]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)
# Sign test
cond <- sum(tel$Bills <= 9.999)
binom.test(cond, length(tel$Bills), alternative = "greater")[[5]]
run <- read.table(file = "run.txt", header = TRUE)
res <- cor.test(run$before,run$after)
res[3]
library("ggpubr", warn.conflicts = FALSE)
softdrink <- subset(run, drink == "lemo")
boxplot(softdrink[,1],softdrink[,2],names=c("Before","After"))
cor.test(softdrink$before,softdrink$after)
energydrink <- subset(run, drink == "energy")
boxplot(energydrink[,1],energydrink[,2],names=c("Before","After"))
cor.test(energydrink$before,energydrink$after)
time_diff <- numeric(length = length(run$before))
for (i in seq_along(time_diff)) {
time_diff[i] <- run$before[i] - run$after[i]
}
time_diff
drink_type <- as.numeric(as.factor(run$drink))
cor.test(time_diff,drink_type)
meatMeal <- chickwts[chickwts$feed == 'meatmeal', ]
sunFlower <- chickwts[chickwts$feed == 'sunflower', ]
options(digits = 3)
# Two samples t-test
t.test(meatMeal$weight, sunFlower$weight)
# Mann-Whitney test
wilcox.test(meatMeal$weight, sunFlower$weight)
# Kolmogorov-Smirnov test
ks.test(meatMeal$weight, sunFlower$weight)
# Means of meatMeal and sunFlower
mM <- mean(meatMeal$weight)
mS <- mean(sunFlower$weight)
# One way ANOVA test
chicaov = lm(weight ~ feed, data = chickwts)
anova(chicaov)
summary(chicaov)$coefficients
# Set the base mean mu1=323.583
mu_hat1 <- 323.583
mu_hat2 <- -163.383 + mu_hat1
mu_hat3 <- -104.833 + mu_hat1
mu_hat4 <- -46.674 + mu_hat1
mu_hat5 <- -77.155 + mu_hat1
mu_hat6 <- 5.333 + mu_hat1
# permutation tests for independent samples
attach(chickwts, warn.conflicts = FALSE)
mystat = function(x) sum(residuals(x)^2)
B = 1000
tstar = numeric(B)
for (i in 1:B) {
feedtstar = sample(feed)
tstar[i] = mystat(lm(weight ~ feedtstar, data = chickwts))
}
myt = mystat(lm(weight ~ feed, data = chickwts))
hist(tstar)
myt
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
2 * pl
# the Kruskal-Wallis test
attach(chickwts, warn.conflicts = FALSE); kruskal.test(weight, feed)
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
print("The p-value from sign test is ...")
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
print("The p-value from sign test is ...")
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)[2]
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
print("The p-value from sign test is ...")
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)[3]
time_diff <- numeric(length = length(run$before))
for (i in seq_along(time_diff)) {
time_diff[i] <- run$before[i] - run$after[i]
}
time_diff
drink_type <- as.numeric(as.factor(run$drink))
cor.test(time_diff,drink_type)[3]
time_diff <- numeric(length = length(run$before))
for (i in seq_along(time_diff)) {
time_diff[i] <- run$before[i] - run$after[i]
}
time_diff
drink_type <- as.numeric(as.factor(run$drink))
print("The p-value is from the Correlation Test")
cor.test(time_diff,drink_type)[3]
time_diff <- numeric(length = length(run$before))
for (i in seq_along(time_diff)) {
time_diff[i] <- run$before[i] - run$after[i]
}
time_diff
drink_type <- as.numeric(as.factor(run$drink))
print("The p-value is from the Correlation Test ...")
cor.test(time_diff,drink_type)[3]
softdrink <- subset(run, drink == "lemo")
boxplot(softdrink[,1],softdrink[,2],names=c("Before","After"))
print("The p-value is from the Correlation Test ...")
cor.test(softdrink$before,softdrink$after)[3]
energydrink <- subset(run, drink == "energy")
boxplot(energydrink[,1],energydrink[,2],names=c("Before","After"))
print("The p-value is from the Correlation Test ...")
cor.test(energydrink$before,energydrink$after)[3]
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)[3]
# Sign test
cond <- sum(tel$Bills <= 9.999)
binom.test(cond, length(tel$Bills), alternative = "greater")[3]
# First off, the measurements have to be transformed to km/sec following the calculation below:
data_mich_1879_km_s = data_mich_1879 + 299000
data_mich_1882_km_s = data_mich_1882 + 299000
data_newcomb_km_s = 7.442/(((data_newcomb/1000)+24.8)/1000000)
# Lastly, we calculate the confidence intervals for the speed of light in km/sec for all three data sets using the One Sample t-test:
t.test(data_mich_1879_km_s)[[4]]; t.test(data_mich_1882_km_s)[[4]]; t.test(data_newcomb_km_s)[[4]]
# Estimated lambda
es_lambda = 1 / sm.avg
es_lambda
knitr::opts_chunk$set(fig.height = 3)
# Retrieve p-values from t-test
pvalue <- function(n, m, mu, nu, sd, B=1000){
# initialize p array
p = numeric(length = B)
# 1000 sample loop
for (b in 1:B) {
# generate x and y
x = rnorm(n, mean = mu, sd = sd); y = rnorm(m, mean = nu, sd = sd)
p[b] = t.test(x, y)[[3]]
}
return(p)
}
# Power of t-test
pow <- function(pvalues, percentage = 0.05) {
return(mean(pvalues < percentage))
}
# Retrieve power of t-test from numerous nu values
powersFromDifferentNuVals <- function(n, m, mu, nu, sd){
# Set empty vector for every nu trial
powers = numeric(length = length(nu))
# Loop thru nu values
for (i in 1:length(nu)) {
# Calculate the p value
p <- pvalue(n, m, mu, nu[i], sd)
# Get powers from different t-test
powers[i] = pow(pvalues = p)
}
return(powers)
}
n = m = 30; mu = 180; sd = 5
# Assign nu
nu = seq(175, 185, by=0.25)
# Calculation of the power
secA <- powersFromDifferentNuVals(n, m, mu, nu, sd)
n=m=100; mu=180; sd=5
# Repeat the preceding exercise.
# Calculation of the power
secB <- powersFromDifferentNuVals(n, m, mu, nu, sd)
n=m=30; mu=180; sd=15
# Repeat the preceding exercise.
# Calculation of the power
secC <- powersFromDifferentNuVals(n, m, mu, nu, sd)
# Set aligning for 3 different histograms
par(mfrow=c(1,3))
# X axis label
xlab <- "mean(p < 0.05)"
# Plot histograms
hist(secA, freq = FALSE,
main = "Histogram of p, sd=5\nn=m=30",
xlab = xlab)
hist(secB, freq = FALSE,
main = "Histogram of p, sd=5\nn=m=100",
xlab = xlab)
hist(secC, freq = FALSE,
main = "Histogram of p, sd=15\nn=m=30",
xlab = xlab)
# Read files, need to use filling for light1882 as the last row
# contains less values than the others
mich_1879 = data.matrix(read.table("light1879.txt"))
mich_1882 = data.matrix(read.table("light1882.txt", fill=TRUE))
newcomb = data.matrix(read.table("light.txt"))
# We determine normality by plotting a histogram and QQ Plot for every dataset.
par(mfrow=c(1,2))
hist(mich_1879, main="Historgram of data:\nMichelson 1879")
qqnorm(mich_1879, main="Q-Q Plot of data:\nMichelson 1879")
par(mfrow=c(1,2))
hist(mich_1882, main="Historgram of data:\nMichelson 1882")
qqnorm(mich_1882, main="Q-Q Plot of data:\nMichelson 1882")
par(mfrow=c(1,2))
hist(newcomb, main="Historgram of data:\nNewcomb")
qqnorm(newcomb, main="Q-Q Plot of data:\nNewcomb")
mich_1879_km_s = mich_1879 + 299000
mich_1882_km_s = mich_1882 + 299000
newcomb_km_s = 7.442/(((newcomb/1000)+24.8)/1000000)
t.test(mich_1879_km_s)[[4]]
t.test(mich_1882_km_s)[[4]]
t.test(newcomb_km_s)[[4]]
speed_of_light = 299792.458
# Extracting upper and lower confidence bounds
# by indexing the One Sample t-test results
t.test(mich_1879_km_s)[[4]][1]<=speed_of_light & speed_of_light<=t.test(mich_1879_km_s)[[4]][2]
t.test(mich_1882_km_s)[[4]][1]<=speed_of_light & speed_of_light<=t.test(mich_1882_km_s)[[4]][2]
t.test(newcomb_km_s)[[4]][1]<=speed_of_light &     speed_of_light<=t.test(newcomb_km_s)[[4]][2]
# Read telephone.txt
tel <- read.table(file = "telephone.txt", header = TRUE)
# Divide row 2 for histogram and boxplot
par(mfrow=c(1,3))
# Plotting telephone.txt
hist(x = tel$Bills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
boxplot(x = tel$Bills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
qqnorm(tel$Bills)
# Extract zero bills from the data
nonZeroBills <- tel[tel$Bills != 0, ]
# Divide row 2 for histogram and boxplot
par(mfrow=c(1,3))
# Plotting telephone.txt
hist(x = nonZeroBills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
boxplot(x = nonZeroBills,
main = "Telephone Bills\n[Original]",
xlab = "Bill")
qqnorm(nonZeroBills)
# We use as test statistic the median of the data
t = median(x=tel$Bills)
print(paste("The median equals", t))
# Set the simulation repeat
B = 1000
# Create T* array
tstar = numeric(length = B)
# Assign sample length
n = length(tel$Bills)
# Assign lambda vector
lamb = sample(c(0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07,
0.08, 0.9, 0.1), replace = TRUE, size = n)
# Simulate B times
for (i in 1:B) {
# Get sample from Exp(lambda)
xstar = rexp(n, rate = sample(lamb, 1))
# Receives tstar by the test statistics the median
tstar[i] = median(xstar)
}
# Draw histogram
hist(tstar, prob=T, col = "grey",
main = "Histogram of T*",
xlab = "T* [T star]")
# Draw density curve of T
lines(density(tstar), col="blue", lwd=2) # add a density estimate with defaults
lines(density(tstar, adjust=2), lty="dotted", col="darkgreen", lwd=2)
axis(1, t, expression(paste("t") ) )
# P left
pl = sum(tstar < t) / B
# P right
pr = sum(tstar > t) / B
p = 2 * min(pl, pr)
print(paste("The p value is", p, "and H0 is not rejected"))
confLvl <- t.test(tel$Bills)[[4]]
print(paste("%95 confidence interval for the sample: [",
confLvl[1], ",", confLvl[2], "]"))
# Align two histogram
par(mfrow = c(1,2))
# Distribution type
lambda <- 0.2
# Sample size
n <- 150
# Trials
rows <- 1000
# Simulate
sim <- rexp(n*rows, lambda)
# Plot histogram
hist(sim, main = "Exp(0.2) Distribution",
xlab = "Value")
# Create matrix
m <- matrix(sim, rows)
# Calculate sample means
sample.means <- rowMeans(m)
# Calculate mean and standard deviation
# of the simulation for CLT(Central Limit Theorem)
sm.avg <- mean(sample.means); sm.sd <- sd(sample.means)
# Plot average
hist(sample.means,
main = "Central Limit Theorem\nfor the sample mean",
xlab = "Means")
# Theoretical(Expected) standard deviation
sm.sd.clt <- sqrt(lambda/n)
# Average of the simulation, standard deviation
# and expected standard deviation
sm.avg; sm.sd; sm.sd.clt
# Calculation of %95 confidence level for the mean
confLvl <- t.test(sample.means)[[4]]
print(paste("%95 confidence interval for the sample: [",
confLvl[1], ",", confLvl[2], "]"))
# Estimated lambda
es_lambda = 1 / sm.avg
# Get sum where bills are >= 40
cond <- sum(tel$Bills >= 40)
# Sign test
binom.test(cond, length(tel$Bills), alternative = "less")[3]
# Wilcoxon test
wilcox.test(tel$Bills, mu=40)[3]
# Sign test
cond <- sum(tel$Bills <= 9.999)
binom.test(cond, length(tel$Bills), alternative = "greater")[3]
run <- read.table(file = "run.txt", header = TRUE)
res <- cor.test(run$before,run$after)
res[3]
library("ggpubr", warn.conflicts = FALSE)
softdrink <- subset(run, drink == "lemo")
boxplot(softdrink[,1],softdrink[,2],names=c("Before","After"))
cor.test(softdrink$before,softdrink$after)[3]
energydrink <- subset(run, drink == "energy")
boxplot(energydrink[,1],energydrink[,2],names=c("Before","After"))
cor.test(energydrink$before,energydrink$after)[3]
time_diff <- numeric(length = length(run$before))
for (i in seq_along(time_diff)) {
time_diff[i] <- run$before[i] - run$after[i]
}
time_diff
drink_type <- as.numeric(as.factor(run$drink))
cor.test(time_diff,drink_type)[3]
meatMeal <- chickwts[chickwts$feed == 'meatmeal', ]
sunFlower <- chickwts[chickwts$feed == 'sunflower', ]
options(digits = 3)
# Two samples t-test
t.test(meatMeal$weight, sunFlower$weight)
# Mann-Whitney test
wilcox.test(meatMeal$weight, sunFlower$weight)
# Kolmogorov-Smirnov test
ks.test(meatMeal$weight, sunFlower$weight)
# Means of meatMeal and sunFlower
mM <- mean(meatMeal$weight)
mS <- mean(sunFlower$weight)
# One way ANOVA test
chicaov = lm(weight ~ feed, data = chickwts)
anova(chicaov)
summary(chicaov)$coefficients
# Set the base mean mu1=323.583
mu_hat1 <- 323.583
mu_hat2 <- -163.383 + mu_hat1
mu_hat3 <- -104.833 + mu_hat1
mu_hat4 <- -46.674 + mu_hat1
mu_hat5 <- -77.155 + mu_hat1
mu_hat6 <- 5.333 + mu_hat1
# permutation tests for independent samples
attach(chickwts, warn.conflicts = FALSE)
mystat = function(x) sum(residuals(x)^2)
B = 1000
tstar = numeric(B)
for (i in 1:B) {
feedtstar = sample(feed)
tstar[i] = mystat(lm(weight ~ feedtstar, data = chickwts))
}
myt = mystat(lm(weight ~ feed, data = chickwts))
hist(tstar)
myt
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
2 * pl
# the Kruskal-Wallis test
attach(chickwts, warn.conflicts = FALSE); kruskal.test(weight, feed)
