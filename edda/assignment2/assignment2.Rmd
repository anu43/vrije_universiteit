---
title: "assignment1"
author: "nihat uzunalioglu - 2660298, emiel kempen - 2640580, saurabh jain - 2666959"
date: "2/26/2020"
output: pdf_document
---

```{r setup, set.seed(43), include=FALSE}
knitr::opts_chunk$set(fig.height = 2.5)
options(show.signif.stars=FALSE)
```

## Exercise 1
```{r, echo=FALSE}
bread = read.table(file = 'data/bread.txt', header = TRUE)
```
a) The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions. Present an R-code for this randomization process.
```{r, echo=TRUE}
# The randomization process for 18 slices
# Take hours column from the data
hrs = as.vector(as.matrix(bread$hours))
# Create environment column
env = rep(c('cold', 'intermediate', 'warm'), each = 6)
# Create humidity column
humi = rep(c('dry', 'wet'), each = 3)
# Converting to data frame
head(data.frame(cbind(hrs, env, humi)))
```

b) Make two boxplots of hours versus the two factors and two interaction plots (keeping the two factors fixed in turn).
```{r, fig.height=4,echo=FALSE}
# Divide into two columns
par(mfrow=c(1, 2))
attach(bread)
# Boxplot vs interaction plot in hours~environment
boxplot(hours~environment, data = bread)
interaction.plot(environment, humidity, hours, fixed = TRUE)
# Boxplot vs interaction plot in hours~humidity
boxplot(hours~humidity, data = bread)
interaction.plot(humidity, environment, hours, fixed = TRUE)
```

c) Perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction. Describe the interaction effect in words.

```{r, echo=TRUE}
# Creating linear model and ANOVA test
breadaov = lm(hours~environment*humidity, data = bread); anova(breadaov)
p_interaction = anova(breadaov)$Pr[3]
```
- The p-value for testing for $H_{0}$:$\gamma_{i,j}$ = 0 for all i, j is `r p_interaction`. Therefore, we reject the null hypothesis $H_{0}$ which means the interaction between environment and humidity is significant for this dataset.

d) Which of the two factors has the greatest (numerical) influence on the decay? Is this a good question?
```{r, echo=TRUE}
summary(breadaov)[4]
```

- When we look up to the variance analysis results, `r bread[7, ]` which corresponds to an environment with intermediate and dry has the most decaying effect in the dataset.

e) Check the model assumptions by using relevant diagnostic tools. Are there any outliers?

```{r, fig.height=4, echo=TRUE}
par(mfrow=c(2, 2))
# Plot the linear fitted model graphs
plot(breadaov)
```

- According to the tables we can say that `r bread[7,]` and `r bread[8,]` are the two that can be considered as outliers.

## Exercise 2

```{R, echo=FALSE}
search <- read.table("data/search.txt", header=TRUE)
```

a) Number the selected students 1 to 15 and show how (by using R) the students could be randomized to the interfaces in a randomized block design.

```{R, echo=TRUE}
N <- as.vector(as.matrix(search$time))
I <- as.vector(as.matrix(search$skill))
B <- as.vector(as.matrix(search$interface))
for (i in 1:B){
  print(sample(1:(N*I)))
}
# xtabs(time~interface+skill,data=search)
```

b) Make some graphical summaries of the data. Are any interactions between interface and skill apparent?

```{R, echo=TRUE}
# interaction.plot(search_data$interface, search_data$skill, search_data$time)
# boxplot(search_data$interface, search_data$skill, search_data$time)

attach(search)
# par(mfrow=c(1,2))
# boxplot(time~interface)
# boxplot(time~skill)

par(mfrow=c(1,2))
interaction.plot(interface,skill,time)
interaction.plot(skill,interface,time)
```
- The pattern ($\alpha1, \alpha2, . . . , \alpha_I$) of treatment effects is assumed to be the same within every block. However, the lines in the seperate interaction plots do not seem to follow similar paths. Therefore, we can assume that there is interaction between interface and skill.

c) Test the null hypothesis that the search time is the same for all interfaces. Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 2.

```{R,echo=TRUE}
search$skill = as.factor(search$skill)
search$interface = as.factor(search$interface)

aovsearch=lm(time~interface+skill, data=search); anova(aovsearch)
```

- From the ANOVA test follows a p-value for interface of `r anova(aovsearch)[[5]][1]`. This indicates that we can reject that the null hypothesis $H_0$, that stated that the mean of the interfaces are the same.

```{r, echo=TRUE}
summary(aovsearch)
```

- Data are assumed to follow the model $Y_{i,b,n} = \mu + \alpha_i + \beta_b + e_{i,b,n}$. Filling in for skill level 3 and interface 2: $Y_{2,3} = 15.013 + 2.700 + 3.033 =$ `r 15.013 + 2.700 + 3.033`$s$. This is the estimated time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 2.

d) Check the model assumptions by using relevant diagnostic tools.

```{R,echo=TRUE}
qqnorm(residuals(aovsearch));qqline(residuals(aovsearch))
```

- The QQ-plot seems to deviate a bit from a straight line in the extremes, but the data can be assumed to be normally distributed.

```{R,echo=TRUE}
plot(fitted(aovsearch),residuals(aovsearch))
```

- The scatter plot shows that the residuals are not. It can therefore be assumed that the populations have equal variances.

e) Perform the non-parametric Friedman test to test whether there is an effect of interface.

```{R, echo=TRUE}
friedman.test(time,interface,skill)
```
- We reject $H_0$ (= interface does not have an effect) as the p-value is < 5%.

f) Test the null hypothesis that the search time is the same for all interfaces by a one-wayANOVA test, ignoring the variable `skill`. Is it right/wrong or useful/not useful to perform this test on this dataset? What assumption on the way the data were obtained is necessary for this test to be valid, and was this assumption met?

```{R, echo=TRUE}
aovsearch2=lm(time~interface, data=search); anova(aovsearch2)
```

- The one-wayANOVA, ignoring the variable `skill`, outputs a p-value of `r anova(aovsearch2)[[5]][1]`. Therefore, we cannot reject $H_0$, meaning that the means are the same for the different interfaces.
- As we the interaction plots in question 2b) showed that there is interaction between interface and skill, it is not right nor usefull to ignore the variable skill.
- The assumption of a one-wayANOVA is that the data is normally distrubuted. However, the QQ-plot below shows that the data is not normal, so the assumption is not met.

```{R, echo=TRUE}
qqnorm(residuals(aovsearch2))
qqline(residuals(aovsearch2))
```

## Exercise 3

``` {r, echo=TRUE}
library(multcomp)
library(lme4)
cow <- read.table("data/cow.txt", header=TRUE)
```

a) Test whether the type of feedingstuffs innfluences milk production using an ordinary "fixed effects" model, fitted with `lm`. Estimate the difference in milk production.

``` {r, echo=TRUE}
cow$id=factor(cow$id); cow$per=factor(cow$per)
aovcow=lm(milk~id+per+treatment,data=cow)
anova(aovcow)
```

- The ANOVA outputs a p-value of `r anova(aovcow)[[5]][3]`, which means that we cannot reject the hypothesis that treatment does not influence the milk production ($H_0$).

```{r, echo=TRUE}
cowlm <- lm(milk~treatment+per+id,data=cow)
(diff_milk <- glht(cowlm,linfct=mcp(treatment="Tukey")))
```

- 

b) Repeat a) and b) by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function lmer). Compare your results to the results found by using a fixed effects model. (You will need to install the R-package lme4, which is not included in the standard distribution of R.)

``` {r, echo=TRUE}
attach(cow)
cowlmer1 <- lmer(milk~treatment+order+per+(1|id),REML=FALSE)
cowlmer2 <- lmer(milk~order+per+(1|id),REML=FALSE)
anova(cowlmer2,cowlmer1)
```

```{r, echo=TRUE}
(diff_milklmer =glht(cowlmer1,linfct=mcp(treatment="Tukey")))
```

c) Study the commands:

` > attach(cow)`
` > t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)`

Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?

```{r, echo=TRUE}
attach(cow)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```
- The t-test outputs the p-value `r t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)[3]`, we therefore cannot reject $H_0$ that there is no difference in milk production. This is indeed compatible with 3a), where we concluded that the treatment did not influence the milk production. In other words, the difference of the means of milk production between both treatments is 0. This t-test is thus a valid test.

## Exercise 4

```{r}
nauseatable = read.table(file = 'nauseatable.txt', header = TRUE)
```

a) Make a data.frame in R consisting of two columns and 304 rows. One column should contain an indicator whether or not the patient in that row suffered from nausea, and the other column should indicate the medicin. Make sure these columns match correctly. Study the outcome of xtabs(∼medicin+naus).

```{r}
table_to_vector = unlist(nauseatable,  use.names = FALSE)
# Create nausea column, possible values 0 (No Nausea), 1(Nausea)
nausea = rep(c('0', '1'), each = 3, times = c(table_to_vector))
med = c('Chlorpromazine','Pentobarbital(100mg)','Pentobarbital(150mg)')
# Create mdecine column, contains name of all the medicines
medicine = rep(c(med,med), each = 1, times = c(table_to_vector))
df = data.frame(cbind(nausea, medicine))
xtabs(~medicine+nausea)
```

b) Perform a permutation test in order to test whether the different medicins work equally well against nausea. Permute the medicin labels for this purpose. Use as test statistic the chisquare test statistic for contingency tables, which can be extracted from the output of the command chisq.test: chisq.test(xtabs(∼medicin+nausea))[[1]].

```{r}
#options(scipen = 999)
nausea_yes <- df[df$nausea == "1",]

meds = factor(nausea_yes$medicine)
mystat=function(x) sum(residuals(x)^2)
B=1000
tstar=numeric(B)
for (i in 1:B) {
  treatstar=sample(meds)
  tstar[i]=mystat(lm(as.numeric(as.character(nausea_yes$nausea))~treatstar))
}
myt=mystat(lm(as.numeric(as.character(nausea_yes$nausea))~treatstar))
myt
hist(tstar)
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
pl
2*pl

chisq.test(xtabs(~medicine+nausea))[[1]]
```

c) Compare the p-value found by the permutation test with the p-value found from the chisquare test for contingency tables. Explain the difference/equality of the two p-values.


## Exercise 5

```{r}
expenses_crime = read.table(file = 'expensescrime.txt', header = TRUE)
```

b) Fit a linear regression model to the data. Use both the step-up and the step-down method to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice.

In the dataset there are 5 possible explantory variables, bad, crime, lawyers, employ, and pop.
```{r}
# Step Up method
summary(lm( expend~bad ,data = expenses_crime))
summary(lm( expend~crime ,data = expenses_crime))
summary(lm( expend~lawyers ,data = expenses_crime))
summary(lm( expend~employ ,data = expenses_crime))
summary(lm( expend~pop ,data = expenses_crime))
```
Explanotory variable 'employ' delivers the highest R^2 value.

```{r}
summary(lm( expend~employ+bad ,data = expenses_crime))
summary(lm( expend~employ+crime ,data = expenses_crime))
summary(lm( expend~employ+lawyers ,data = expenses_crime))
summary(lm( expend~employ+pop ,data = expenses_crime))
```
Newly added variable 'lawyers' yields better R^2 compared to others.

```{r}
summary(lm( expend~employ+lawyers+bad ,data = expenses_crime))
summary(lm( expend~employ+lawyers+crime ,data = expenses_crime))
summary(lm( expend~employ+lawyers+pop ,data = expenses_crime))
```
Adding additional variables leads to insignificant explanatory variables. Thus, 'step up' process need to be stopped at previous step.

```{r}
#Final model for the step up approach
summary(lm( expend~employ+lawyers ,data = expenses_crime))
```
Step Down method

```{r}
summary(lm( expend~bad+crime+lawyers+employ+pop ,data = expenses_crime))
```
Explanatory variable 'bad' has p-value is larger than 0.05. Thus removing it from the model.
```{r}
summary(lm( expend~crime+lawyers+employ+pop ,data = expenses_crime))

```
Explanatory variable 'crime' and 'pop' has p-value is larger than 0.05. Thus removing them from the model.
```{r}
#Final model using Step down method
summary(lm( expend~lawyers+employ ,data = expenses_crime))

```
No need to remove further variables as all remaining explanatory variables in the model are significant.
Conclusion: "Step up' and 'Step down' methods results into same model with R-squared:  0.9632.

a) Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.
```{r}
plot(expenses_crime[,c(3:7)])
```

In the plot below, there's are 'potential points' on x axis between values 70000-85000.
```{r}
plot(expenses_crime$lawyers, expenses_crime$employ)

```

In the following plot, we clearly see an influence point: the Cook’s distance is 5.47 for the leverage point at index 5 and 6.38 for the leverage point at index 8.
```{r}
model = lm( expend~lawyers+employ ,data = expenses_crime)
round(cooks.distance(model),2)
plot(cooks.distance(model),type="b")
```

In the following graph and correlation table, 'pop' and 'employ' are collinear with correlation value of 0.9707407.
```{r}
cor(expenses_crime[,c(3:7)])
plot(expenses_crime$pop, expenses_crime$employ)
```


VIF values of both the variables in the model is higher than 5, which represents the collinearity problem. Also after adding highly correlational varioble to the model, VIF of the model goes up.
```{r}
install.packages("car")
library(car)

model1 = lm( expend~lawyers+employ+pop ,data = expenses_crime)
vif(model)
vif(model1)
```

c) Check the model assumptions by using relevant diagnostic tools.

Scatter plot of residuals against each Xk in the model separately.
```{r}
lw_model = lm( expend~lawyers ,data = expenses_crime)
plot(residuals(lw_model),expenses_crime$lawyers)

em_model = lm( expend~employ ,data = expenses_crime)
plot(residuals(em_model),expenses_crime$employ)
```

From the normal QQ-plot of the residuals it is evident that error is not normally distributed.

```{r}
qqnorm(residuals(model))
```