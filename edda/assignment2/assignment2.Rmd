---
title: "assignment1"
author: "nihat uzunalioglu - 2660298, emiel kempen - 2640580, saurabh jain - 2666959"
date: "2/26/2020"
output: pdf_document
---

```{r setup, set.seed(43), include=FALSE}
knitr::opts_chunk$set(fig.height = 3)
options(show.signif.stars=FALSE)
```

## Exercise 1
```{r, echo=FALSE}
bread = read.table(file = 'data/bread.txt', header = TRUE)
```
a) 

```{r, echo=TRUE}
# The randomization process for 18 slices
N=3; I=2; J=3
rbind(rep(1:I, each = N*J), rep(1:J, N*I), sample(1:(N*I*J)))
```
N: the number of units for each combination
I: the levels of humidity
J: the levels of environment

We performed randomization according to the levels of the dataset. This way we can randomly assign slices of bread to the different levels of the dataset.

b)
```{r, fig.height=4,echo=FALSE}
# Divide into two columns
par(mfrow=c(1, 2))
attach(bread)
# Boxplot vs interaction plot in hours~environment
boxplot(hours~environment, data = bread)
interaction.plot(environment, humidity, hours, fixed = TRUE)
# Boxplot vs interaction plot in hours~humidity
boxplot(hours~humidity, data = bread)
interaction.plot(humidity, environment, hours, fixed = TRUE)
```

c)

```{r, echo=TRUE}
# Creating linear model and ANOVA test
# Factorization
bread$humidity = as.factor(bread$humidity)
bread$environment = as.factor(bread$environment)
breadaov = lm(hours~environment*humidity, data = bread); anova(breadaov)
p_interaction = anova(breadaov)$Pr[3]
```
- The p-value for testing for $H_{0}$:$\gamma_{i,j}$ = 0 for all i, j is `r p_interaction`. Therefore, we reject the null hypothesis $H_{0}$ which means the interaction between environment and humidity is significant for this dataset.

d) 
```{r, echo=TRUE}
contrasts(bread$humidity)=contr.sum
contrasts(bread$environment)=contr.sum
breadaov2 = lm(hours~humidity*environment,data=bread)
summary(breadaov)[4]
```

- Here we can see that the environment factor affects the hours in a bigger way than humidity. We can see changes of $149.33$ and $-64.66$ in comparison with $38.667$.

- We don't think it's a good question though, because we agree that the root of impact lies in the relationship between these two variables rather than just one of them, even though it indicates bigger changes.

e)

```{r, fig.height=4, echo=TRUE}
par(mfrow=c(2, 2))
# Plot the linear fitted model graphs
plot(breadaov)
```

- According to the tables we can say that `r bread[7,]` and `r bread[8,]` are the two that can be considered as outliers.

## Exercise 2

```{r}
search = read.table("data/search.txt", header=TRUE)
```

a)

```{r, results = 'hide'}
N = 1 #
I = 3 
B = 5
for (i in 1:B){
  print(sample(1:(N*I)))
}
```

- The blocks created represent the students grouped per skill-level, so totaling to 5 blocks of 3 students each. For block 1 assign student 1 to interface 1, student 3 to interface 2, etc., for block 2 assign student 1 to interface 1, student 2 to interface 2, etc.

b)

```{R, echo=TRUE}

attach(search)

par(mfrow=c(1,2))
interaction.plot(interface,skill,time)
interaction.plot(skill,interface,time)
```
- The pattern ($\alpha1, \alpha2, . . . , \alpha_I$) of treatment effects is assumed to be the same within every block. However, the lines in the seperate interaction plots do not seem to be parallel. Therefore, we can assume that there is an interaction between interface and skill.

c)

```{r, results = 'hide'}
search$skill = as.factor(search$skill)
search$interface = as.factor(search$interface)

aovsearch=lm(time~interface+skill, data=search); anova(aovsearch)
```

- From the ANOVA test follows a p-value for interface of `r anova(aovsearch)[[5]][1]`. This indicates that we can reject that the null hypothesis $H_0$, that stated that the means of the search times for all interfaces is the same.

```{r, echo=TRUE, results = 'hide'}
summary(aovsearch)[4]
```

- Data are assumed to follow the model $Y_{i,b,n} = \mu + \alpha_i + \beta_b + e_{i,b,n}$. Filling in for skill level 3 and interface 2: $Y_{2,3} = 15.013 + 2.700 + 3.033 =$ `r 15.013 + 2.700 + 3.033` $s$. This is the estimated time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 2.

d)

```{R,echo=TRUE}
qqnorm(residuals(aovsearch));qqline(residuals(aovsearch))
```

- The QQ-plot seems to deviate a bit from a straight line in the extremes, but the residuals can be assumed to be normally distributed.

```{R,echo=TRUE}
plot(fitted(aovsearch),residuals(aovsearch))
```

- The scatter plot shows no clear pattern, so the residuals are (almost) symmetrically distributed.

e)

```{R, echo=TRUE, results = 'hide'}
friedman.test(time,interface,skill)
```
- We reject $H_0$ (= interface does not have an effect) as the p-value is `r friedman.test(time,interface,skill)[3]`, which is lower than 5%.

f)

```{r, echo=TRUE, results = 'hide'}
aovsearch2 = lm(time~interface, data=search); anova(aovsearch2)
```

- The one-wayANOVA, ignoring the variable `skill`, outputs a p-value of `r anova(aovsearch2)[[5]][1]`. Therefore, we cannot reject $H_0$, meaning that the means of the search times are the same for the different interfaces.
- As we the interaction plots in question 2b) showed that there is interaction between interface and skill, it is not right nor useful to ignore the variable `skill`.
- The assumption of a one-wayANOVA is that the data is normally distrubuted. However, the QQ-plot below shows that the data is not normal, so the assumption is not met, nor is it valid.

```{R, echo=TRUE}
qqnorm(residuals(aovsearch2))
qqline(residuals(aovsearch2))
```

## Exercise 3

``` {r, echo=FALSE, results = 'hide'}
library(lme4)
```

```{r, echo=TRUE}
cow = read.table("data/cow.txt", header=TRUE)
```

a)

``` {r, echo=TRUE, results = 'hide'}
aovcow = lm(milk~id+per+treatment,data=cow)
anova(aovcow)
```

- The factor of interest here is type of feedingstuffs (treatment), which is therefore put in as the last factor of the ANOVA formula. The ANOVA outputs a p-value of `r anova(aovcow)[[5]][3]`, which means we accept the null hypothesis that treatment does not influence the milk production ($H_0$).

```{r, echo=TRUE, results = 'hide'}
cow$id = factor(cow$id); cow$per=factor(cow$per)
cowlm = lm(milk~treatment+per+id, data=cow)
summary(cowlm)
```

- The difference between treatment A (the Intercept) and treatment B is `r summary(cowlm)[[4]][2]`.

b)

``` {r, echo=TRUE, results = 'hide'}
attach(cow)
cowlmer1 = lmer(milk~treatment+order+per+(1|id),REML=FALSE)
cowlmer2 = lmer(milk~order+per+(1|id),REML=FALSE)
anova(cowlmer2,cowlmer1)
```

- By performing a mixed effects analysis in the form of an ANOVA test, modelling the cow effect as a random effect using `lmer`, we find that the p-value equals `r anova(cowlmer2,cowlmer1)[[8]][2]`. This leads us to accepting the hypothesis that treatment does not influence the milk production ($H_0$)

```{r, echo=TRUE, results = 'hide'}
summary(cowlmer1)
```

- From the `summary` function, it follows that - just as in question 3a) - the difference between treatment A (the Intercept) and treatment B is `r summary(cowlmer1)[[10]][2]`.

c)

```{r, results='hide'}
attach(cow)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```
- The t-test outputs the p-value `r t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)[3]`, we therefore cannot reject $H_0$ that there is no difference in milk production given the two treatments. This is indeed compatible with 3a), where we concluded that the treatment did not influence the milk production. This t-test is thus a valid test.

## Exercise 4

```{r}
nauseatable = read.table(file = 'data/nauseatable.txt', header = TRUE)
```

a)

```{r}
table_to_vector = unlist(nauseatable,  use.names = FALSE)
# Create nausea column, possible values 0 (No Nausea), 1(Nausea)
nausea = rep(c('0', '1'), each = 3, times = c(table_to_vector))
med = c('Chlorpromazine','Pentobarbital(100mg)','Pentobarbital(150mg)')
# Create medicine column, contains name of all the medicines
medicine = rep(c(med,med), each = 1, times = c(table_to_vector))
df = data.frame(cbind(nausea, medicine))
(df[c(1,101,133,181,233,268),])
```

While studying the outcome of the table below, we see that with xtabs we get a contingency table from the medicine and nausea factors.
There are more people suffering from nausea with the medicine Pentobarbital(100mg and 150mg combined) than with Chlorpromazine
```{r}
xtabs(~medicine+nausea)
```

b)

```{r}
#options(scipen = 999)
meds = factor(medicine)
mystat=function(x) sum(residuals(x)^2)
B=1000
tstar=numeric(B)
for (i in 1:B) {
  treatstar=sample(medicine)
  tstar[i]=chisq.test(xtabs(~treatstar+nausea, data = nauseatable))[[1]]
}
myt=chisq.test(xtabs(~medicine+nausea, data = nauseatable))[[1]]
myt
hist(tstar)
```

```{r}
pl = sum(tstar<myt)/B 
pr = sum(tstar>myt)/B
pmin = min(pl,pr) 
(pvalue = pmin)
```
The obtained p-value from permutation test is over 5% thus we accept the null hypothesis ($H_0$) and conclude that the variables are not dependent.

c)

```{r}
(pvalue_chisq = chisq.test(xtabs(~medicine+nausea, data = nauseatable))[[3]])
(pvalue_tstar=pmin)
```

We received very close p-values from both the permutation and chi-square tests. Even though we received `r (pvalue_tstar=pmin)` from the permutation test, since they both statiscally perform with regards to significance of the factors we accepted the null hypothesis ($H_0$). Since a single chi-squared test is one member from the permutation test, having close result is justifiable.


## Exercise 5

```{r}
expenses_crime = read.table(file = 'data/expensescrime.txt', header = TRUE)
```

b)

In the dataset there are 5 possible explantory `variables`, `bad`, `crime`, `lawyers`, `employ`, and `pop`.
```{r}
# Step Up method
summary(lm( expend~bad ,data = expenses_crime))[[9]]
summary(lm( expend~crime ,data = expenses_crime))[[9]]
summary(lm( expend~lawyers ,data = expenses_crime))[[9]]
summary(lm( expend~employ ,data = expenses_crime))[[9]]
summary(lm( expend~pop ,data = expenses_crime))[[9]]
```
Explanotory variable 'employ' delivers the highest $R^2$ value.

```{r}
summary(lm( expend~employ+bad ,data = expenses_crime))[[9]]
summary(lm( expend~employ+crime ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers ,data = expenses_crime))[[9]]
summary(lm( expend~employ+pop ,data = expenses_crime))[[9]]
```
Newly added variable 'lawyers' yields better $R^2$ compared to others.

```{r}
summary(lm( expend~employ+lawyers+bad ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers+crime ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers+pop ,data = expenses_crime))[[9]]
```
Adding additional variables leads to insignificant explanatory variables. Thus, 'step up' process need to be stopped at previous step.

```{r}
#Final model for the step up approach
summary(lm( expend~employ+lawyers ,data = expenses_crime))[8]
```
Step Down method

```{r}
summary(lm( expend~bad+crime+lawyers+employ+pop ,data = expenses_crime))[[4]]
```
Explanatory variable 'crime' has p-value is larger than 0.05. Thus removing it from the model.
```{r}
summary(lm( expend~bad+lawyers+employ+pop ,data = expenses_crime))[[4]]

```
Explanatory variable 'bad' has p-value is larger than 0.05. Thus removing them from the model.
```{r}
#Final model using Step down method
summary(lm( expend~lawyers+employ+pop ,data = expenses_crime))[[4]]

```
Explanatory variable 'pop' has p-value is larger than 0.05. Thus removing them from the model.

```{r}
summary(lm( expend~lawyers+employ ,data = expenses_crime))[8]

```

No need to remove further variables as all remaining explanatory variables in the model are significant.
Conclusion: "Step up' and 'Step down' methods results into same model with R-squared:  `r summary(lm( expend~lawyers+employ ,data = expenses_crime))[8]`.

a) 
```{r}
plot(expenses_crime[,c(3:7)])
```

In the plot below, there are 'potential points' on x axis between values 70000-85000.
```{r}
plot(expenses_crime$lawyers, expenses_crime$employ)
```


```{r}
model = lm( expend~lawyers+employ ,data = expenses_crime)
library(car)
par(mfrow=c(1, 2))
plot(cooks.distance(model),type="b")
influencePlot(model, main="Influence Plot", sub="Circle size corresponds Cook's Distance")
```
In the plot above, we clearly see influence points: the Cook’s distance is `r round(cooks.distance(model),2)[5]` for the leverage point at index 5 and `r round(cooks.distance(model),2)[8]` for the leverage point at index 8.


In the following graph and correlation table, 'pop' and 'employ' are collinear with correlation value of `r cor(expenses_crime[,c(3:7)])[24]`.
```{r}
cor(expenses_crime[,c(3:7)])[24]
plot(expenses_crime$pop, expenses_crime$employ)
```


VIF values of both the variables in the model is higher than 5, which represents the collinearity problem.
```{r}
vif(model)
```
Since both the variables have same value, we need to remove one of the variables from the model as below:
```{r}
model_new = lm( expend~lawyers ,data = expenses_crime)
```


c)

Scatter plot of residuals against each Xk in the model separately. There is no visible pattern in the plots.
```{r}
lw_model = lm( expend~lawyers ,data = expenses_crime)
em_model = lm( expend~employ ,data = expenses_crime)

par(mfrow=c(1, 2))
plot(residuals(lw_model),expenses_crime$lawyers)
plot(residuals(em_model),expenses_crime$employ)
```

From the normal QQ-plot of the residuals it is evident that error is not normally distributed.

```{r}
qqnorm(residuals(model));qqline(residuals(model))
```
