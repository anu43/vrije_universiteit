---
title: "assignment1"
author: "nihat uzunalioglu - 2660298, emiel kempen - 2640580, saurabh jain - 2666959"
date: "2/26/2020"
output: pdf_document
---

```{r setup, set.seed(43), include=FALSE}
knitr::opts_chunk$set(fig.height = 2.5)
options(show.signif.stars=FALSE)
```

## Exercise 1
```{r, echo=FALSE}
bread = read.table(file = 'data/bread.txt', header = TRUE)
```
a) The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions. Present an R-code for this randomization process.
```{r, echo=TRUE}
# The randomization process for 18 slices
# Take hours column from the data
hrs = as.vector(as.matrix(bread$hours))
# Create environment column
env = rep(c('cold', 'intermediate', 'warm'), each = 6)
# Create humidity column
humi = rep(c('dry', 'wet'), each = 3)
# Converting to data frame
head(data.frame(cbind(hrs, env, humi)))
```

b) Make two boxplots of hours versus the two factors and two interaction plots (keeping the two factors fixed in turn).
```{r, fig.height=4,echo=FALSE}
# Divide into two columns
par(mfrow=c(1, 2))
attach(bread)
# Boxplot vs interaction plot in hours~environment
boxplot(hours~environment, data = bread)
interaction.plot(environment, humidity, hours, fixed = TRUE)
# Boxplot vs interaction plot in hours~humidity
boxplot(hours~humidity, data = bread)
interaction.plot(humidity, environment, hours, fixed = TRUE)
```

c) Perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction. Describe the interaction effect in words.

```{r, echo=TRUE}
# Creating linear model and ANOVA test
breadaov = lm(hours~environment*humidity, data = bread); anova(breadaov)
p_interaction = anova(breadaov)$Pr[3]
```
- The p-value for testing for $H_{0}$:$\gamma_{i,j}$ = 0 for all i, j is `r p_interaction`. Therefore, we reject the null hypothesis $H_{0}$ which means the interaction between environment and humidity is significant for this dataset.

d) Which of the two factors has the greatest (numerical) influence on the decay? Is this a good question?
```{r, echo=TRUE}
summary(breadaov)[4]
```

- When we look up to the variance analysis results, `r bread[7, ]` which corresponds to an environment with intermediate and dry has the most decaying effect in the dataset.

e) Check the model assumptions by using relevant diagnostic tools. Are there any outliers?

```{r, fig.height=4, echo=TRUE}
par(mfrow=c(2, 2))
# Plot the linear fitted model graphs
plot(breadaov)
```

- According to the tables we can say that `r bread[7,]` and `r bread[8,]` are the two that can be considered as outliers.

## Exercise 2

```{r}
search = read.table("data/search.txt", header=TRUE)
```

a) Number the selected students 1 to 15 and show how (by using R) the students could be randomized to the interfaces in a randomized block design.

```{r, results = 'hide'}
N = 1 #
I = 3 
B = 5
for (i in 1:B){
  print(sample(1:(N*I)))
}
```

- The blocks created represent the students grouped per skill-level, so totaling to 5 blocks of 3 students each. For block 1 assign student 1 to interface 1, student 3 to interface 2, etc., for block 2 assign student 1 to interface 1, student 2 to interface 2, etc.

b) Make some graphical summaries of the data. Are any interactions between interface and skill apparent?

```{R, echo=TRUE}

attach(search)

par(mfrow=c(1,2))
interaction.plot(interface,skill,time)
interaction.plot(skill,interface,time)
```
- The pattern ($\alpha1, \alpha2, . . . , \alpha_I$) of treatment effects is assumed to be the same within every block. However, the lines in the seperate interaction plots do not seem to be parallel. Therefore, we can assume that there is an interaction between interface and skill.

c) Test the null hypothesis that the search time is the same for all interfaces. Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 2.

```{r, results = 'hide'}
search$skill = as.factor(search$skill)
search$interface = as.factor(search$interface)

aovsearch=lm(time~interface+skill, data=search); anova(aovsearch)
```

- From the ANOVA test follows a p-value for interface of `r anova(aovsearch)[[5]][1]`. This indicates that we can reject that the null hypothesis $H_0$, that stated that the means of the search times for all interfaces is the same.

```{r, echo=TRUE}
summary(aovsearch)[4]
```

- Data are assumed to follow the model $Y_{i,b,n} = \mu + \alpha_i + \beta_b + e_{i,b,n}$. Filling in for skill level 3 and interface 2: $Y_{2,3} = 15.013 + 2.700 + 3.033 =$ `r 15.013 + 2.700 + 3.033` $s$. This is the estimated time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 2.

d) Check the model assumptions by using relevant diagnostic tools.

```{R,echo=TRUE}
qqnorm(residuals(aovsearch));qqline(residuals(aovsearch))
```

- The QQ-plot seems to deviate a bit from a straight line in the extremes, but the residuals can be assumed to be normally distributed.

```{R,echo=TRUE}
plot(fitted(aovsearch),residuals(aovsearch))
```

- The scatter plot shows no clear pattern, so the residuals are (almost) symmetrically distributed.

e) Perform the non-parametric Friedman test to test whether there is an effect of interface.

```{R, echo=TRUE}
friedman.test(time,interface,skill)
```
- We reject $H_0$ (= interface does not have an effect) as the p-value is `r friedman.test(time,interface,skill)[3]`, which is lower than 5%.

f) Test the null hypothesis that the search time is the same for all interfaces by a one-wayANOVA test, ignoring the variable `skill`. Is it right/wrong or useful/not useful to perform this test on this dataset? What assumption on the way the data were obtained is necessary for this test to be valid, and was this assumption met?

```{r, echo=TRUE}
aovsearch2 = lm(time~interface, data=search); anova(aovsearch2)
```

- The one-wayANOVA, ignoring the variable `skill`, outputs a p-value of `r anova(aovsearch2)[[5]][1]`. Therefore, we cannot reject $H_0$, meaning that the means of the search times are the same for the different interfaces.
- As we the interaction plots in question 2b) showed that there is interaction between interface and skill, it is not right nor useful to ignore the variable `skill`.
- The assumption of a one-wayANOVA is that the data is normally distrubuted. However, the QQ-plot below shows that the data is not normal, so the assumption is not met, nor is it valid.

```{R, echo=TRUE}
qqnorm(residuals(aovsearch2))
qqline(residuals(aovsearch2))
```

## Exercise 3

``` {r, echo=FALSE}
library(lme4)
```

```{r, echo=TRUE}
cow = read.table("data/cow.txt", header=TRUE)
```

a) Test whether the type of feedingstuffs influences milk production using an ordinary "fixed effects" model, fitted with `lm`. Estimate the difference in milk production.

``` {r, echo=TRUE}
aovcow = lm(milk~id+per+treatment,data=cow)
anova(aovcow)
```

- The factor of interest here is type of feedingstuffs (treatment), which is therefore put in as the last factor of the ANOVA formula. The ANOVA outputs a p-value of `r anova(aovcow)[[5]][3]`, which means we accept the null hypothesis that treatment does not influence the milk production ($H_0$).

```{r, echo=TRUE}
cow$id = factor(cow$id); cow$per=factor(cow$per)
cowlm = lm(milk~treatment+per+id, data=cow)
summary(cowlm)
```

- The difference between treatment A (the Intercept) and treatment B is `r summary(cowlm)[[4]][2]`.

b) Repeat a) and b) by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function `lmer`). Compare your results to the results found by using a fixed effects model.

``` {r, echo=TRUE}
attach(cow)
cowlmer1 = lmer(milk~treatment+order+per+(1|id),REML=FALSE)
cowlmer2 = lmer(milk~order+per+(1|id),REML=FALSE)
anova(cowlmer2,cowlmer1)
```

- By performing a mixed effects analysis in the form of an ANOVA test, modelling the cow effect as a random effect using `lmer`, we find that the p-value equals `r anova(cowlmer2,cowlmer1)[[8]][2]`. This leads us to accepting the hypothesis that treatment does not influence the milk production ($H_0$)

```{r, echo=TRUE}
summary(cowlmer1)
```

- From the `summary` function, it follows that - just as in question 3a) - the difference between treatment A (the Intercept) and treatment B is `r summary(cowlmer1)[[10]][2]`.

c) Study the commands:

` > attach(cow)`
` > t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)`

Does this produce a valid test for a difference in milk production? Is its conclusion compatible with the one obtained in a)? Why?

```{r, results='hide'}
attach(cow)
t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)
```
- The t-test outputs the p-value `r t.test(milk[treatment=="A"],milk[treatment=="B"],paired=TRUE)[3]`, we therefore cannot reject $H_0$ that there is no difference in milk production given the two treatments. This is indeed compatible with 3a), where we concluded that the treatment did not influence the milk production. This t-test is thus a valid test.

## Exercise 4

```{r}
nauseatable = read.table(file = 'data/nauseatable.txt', header = TRUE)
```

a) Make a data.frame in R consisting of two columns and 304 rows. One column should contain an indicator whether or not the patient in that row suffered from nausea, and the other column should indicate the medicin. Make sure these columns match correctly. Study the outcome of xtabs(∼medicin+naus).

```{r}
table_to_vector = unlist(nauseatable,  use.names = FALSE)
# Create nausea column, possible values 0 (No Nausea), 1(Nausea)
nausea = rep(c('0', '1'), each = 3, times = c(table_to_vector))
med = c('Chlorpromazine','Pentobarbital(100mg)','Pentobarbital(150mg)')
# Create medicine column, contains name of all the medicines
medicine = rep(c(med,med), each = 1, times = c(table_to_vector))
df = data.frame(cbind(nausea, medicine))
```

While studying the outcome of the table below, we see that with xtabs we get a contingency table from the medicine and nausea factors.
There are more people suffering from nausea with the medicine Pentobarbital(100mg and 150mg combined) than with Chlorpromazine
```{r}
xtabs(~medicine+nausea)
```

b) Perform a permutation test in order to test whether the different medicins work equally well against nausea. Permute the medicin labels for this purpose. Use as test statistic the chisquare test statistic for contingency tables, which can be extracted from the output of the command chisq.test: chisq.test(xtabs(∼medicin+nausea))[[1]].

```{r}
#options(scipen = 999)
meds = factor(medicine)
mystat=function(x) sum(residuals(x)^2)
B=1000
tstar=numeric(B)
for (i in 1:B) {
  treatstar=sample(medicine)
  tstar[i]=chisq.test(xtabs(~treatstar+nausea, data = nauseatable))[[1]]
}
myt=chisq.test(xtabs(~medicine+nausea, data = nauseatable))[[1]]
myt
hist(tstar)
```

```{r}
pl = sum(tstar<myt)/B 
pr = sum(tstar>myt)/B
pmin = min(pl,pr) 
(pvalue = pmin)
```
The obtained p-value from permutation test is over 5% thus we accept the null hypothesis ($H_0$) and conclude that the variables are not dependent.

c) Compare the p-value found by the permutation test with the p-value found from the chisquare test for contingency tables. Explain the difference/equality of the two p-values.

```{r}
(pvalue_chisq = chisq.test(xtabs(~medicine+nausea, data = nauseatable))[[3]])
(pvalue_tstar=pmin)
```

We received very close p-values from both the permutation and chi-square tests. Even though we received `r (pvalue_tstar=pmin)` from the permutation test, since they both statiscally perform with regards to significance of the factors we accepted the null hypothesis ($H_0$). Since a single chi-squared test is one member from the permutation test, having close result is justifiable.


## Exercise 5

```{r}
expenses_crime = read.table(file = 'data/expensescrime.txt', header = TRUE)
```

b) Fit a linear regression model to the data. Use both the step-up and the step-down method to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice.

In the dataset there are 5 possible explantory `variables`, `bad`, `crime`, `lawyers`, `employ`, and `pop`.
```{r}
# Step Up method
summary(lm( expend~bad ,data = expenses_crime))[[9]]
summary(lm( expend~crime ,data = expenses_crime))[[9]]
summary(lm( expend~lawyers ,data = expenses_crime))[[9]]
summary(lm( expend~employ ,data = expenses_crime))[[9]]
summary(lm( expend~pop ,data = expenses_crime))[[9]]
```
Explanotory variable 'employ' delivers the highest $R^2$ value.

```{r}
summary(lm( expend~employ+bad ,data = expenses_crime))[[9]]
summary(lm( expend~employ+crime ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers ,data = expenses_crime))[[9]]
summary(lm( expend~employ+pop ,data = expenses_crime))[[9]]
```
Newly added variable 'lawyers' yields better $R^2$ compared to others.

```{r}
summary(lm( expend~employ+lawyers+bad ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers+crime ,data = expenses_crime))[[9]]
summary(lm( expend~employ+lawyers+pop ,data = expenses_crime))[[9]]
```
Adding additional variables leads to insignificant explanatory variables. Thus, 'step up' process need to be stopped at previous step.

```{r}
#Final model for the step up approach
summary(lm( expend~employ+lawyers ,data = expenses_crime))[[9]]
```
Step Down method

```{r}
summary(lm( expend~bad+crime+lawyers+employ+pop ,data = expenses_crime))
```
Explanatory variable 'bad' has p-value is larger than 0.05. Thus removing it from the model.
```{r}
summary(lm( expend~crime+lawyers+employ+pop ,data = expenses_crime))

```
Explanatory variable 'crime' and 'pop' has p-value is larger than 0.05. Thus removing them from the model.
```{r}
#Final model using Step down method
summary(lm( expend~lawyers+employ ,data = expenses_crime))[8]

```
No need to remove further variables as all remaining explanatory variables in the model are significant.
Conclusion: "Step up' and 'Step down' methods results into same model with R-squared:  `r summary(lm( expend~lawyers+employ ,data = expenses_crime))[8]`.

a) Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.
```{r}
plot(expenses_crime[,c(3:7)])
```

In the plot below, there are 'potential points' on x axis between values 70000-85000.
```{r}
plot(expenses_crime$lawyers, expenses_crime$employ)
```


```{r}
model = lm( expend~lawyers+employ ,data = expenses_crime)
round(cooks.distance(model),2)
plot(cooks.distance(model),type="b")
```
In the plot above, we clearly see influence points: the Cook’s distance is `r round(cooks.distance(model),2)[5]` for the leverage point at index 5 and `r round(cooks.distance(model),2)[8]` for the leverage point at index 8.

Influence PLot
```{r}
library(car)
influencePlot(model, main="Influence Plot", sub="Circle size corresponds Cook's Distance")
```

In the following graph and correlation table, 'pop' and 'employ' are collinear with correlation value of `r cor(expenses_crime[,c(3:7)])[24]`.
```{r}
cor(expenses_crime[,c(3:7)])
plot(expenses_crime$pop, expenses_crime$employ)
```


VIF values of both the variables in the model is higher than 5, which represents the collinearity problem. Also after adding highly correlational varioble to the model, VIF of the model goes up.
```{r}
model1 = lm( expend~lawyers+employ+pop ,data = expenses_crime)
vif(model)
vif(model1)
```
Since both the variables have same value, we need to remove one of the variables from the model as below:
```{r}
model_new = lm( expend~lawyers ,data = expenses_crime)
```


c) Check the model assumptions by using relevant diagnostic tools.

Scatter plot of residuals against each Xk in the model separately. There is no visible pattern in the plots.
```{r}
lw_model = lm( expend~lawyers ,data = expenses_crime)
plot(residuals(lw_model),expenses_crime$lawyers)

em_model = lm( expend~employ ,data = expenses_crime)
plot(residuals(em_model),expenses_crime$employ)
```

From the normal QQ-plot of the residuals it is evident that error is not normally distributed.

```{r}
qqnorm(residuals(model));qqline(residuals(model))
```
